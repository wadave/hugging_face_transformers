{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "## BERT Text Classification with Numerical and Categorical Features\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJR6t_gCQe_x"
      },
      "source": [
        "## Introduction\n",
        "---\n",
        "\n",
        "\n",
        "This notebook demonstrates how to integrate numerical and categorical features with text features when using Hugging Face BERT transformer. \n",
        "\n",
        "1. [Dataset](#1-Dataset)  \n",
        "Women's E-Commerce Clothing Reviews data [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) is used in the current notebook.\n",
        "\n",
        "Different methods were tested:\n",
        "\n",
        "2. [Classification by Numerical and Categorical Features Using XGBoost](#2-Classification-by-Numerical-and-Categorical-Features-Using-XGBoost)\n",
        "3. [Classification by Text Only Using Hugging Face BERT Transformer](#3-Classification-by-Text-Only-Using-Hugging-Face-BERT-Transformer)\n",
        "4. [Transform Numerical and Categorical to Text and Use BERT Transformer](#4-Transform-Numerical-and-Categorical-to-Text-and-Use-BERT-Transformer)\n",
        "5. [BERT Text Classification Combining Numerical and Categorical Features](#5-BERT-Text-Classification-Combining-Numerical-and-Categorical-Features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "## Different methods\n",
        "\n",
        "---\n",
        "### Baseline methods\n",
        "\n",
        "- Method 1: \n",
        " [XGBoost](#2-Classification-by-Numerical-and-Categorical-Features-Using-XGBoost)\n",
        "\n",
        "An XGBoost model was built to treat numerical and categorical features only.\n",
        "\n",
        "<img src=\"./img/num.png\" width=\"750\">  \n",
        "\n",
        "- Method 2:\n",
        " [Hugging Face BERT Transformer - Treat text feature only](#3-Classification-by-Text-Only-Using-Hugging-Face-BERT-Transformer)\n",
        "\n",
        "\"BertForSequenceClassification\" model was used to classify the text feature only.\n",
        "\n",
        "<img src=\"./img/txt.png\" width=\"750\">  \n",
        "\n",
        "### Advanced methods\n",
        "\n",
        "- Method 3:\n",
        " [Hugging Face BERT Transformer - Convert numerical and categorical to text and treat all text together](#4-Transform-Numerical-and-Categorical-to-Text-and-Use-BERT-Transformer)\n",
        "\"BertForSequenceClassification\" model was used to classify all the text together.\n",
        "\n",
        "<img src=\"./img/all_txt.png\" width=\"900\">  \n",
        "\n",
        "- Method 4:\n",
        " [Hugging Face BERT Transformer - Convert numerical and categorical to text and treat all text together](#5-BERT-Text-Classification-Combining-Numerical-and-Categorical-Features)\n",
        "\"BertForSequenceClassification\" was modified and concatenated numerical and categorical features to the joined layer and then applied a classifier on the last layer.  \n",
        "\n",
        "<img src=\"./img/txt_num.png\" width=\"900\">  \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary results\n",
        "----\n",
        "\n",
        "The following table shows preliminary results using above-mentioned methods.\n",
        "\n",
        "##### `Please note, due to limited computing power, only 2000 samples and 2 epochs were used. The BERT based model performance could be improved if more data points and epochs were used.`\n",
        "\n",
        "\n",
        "| Method                                                          | F1 Score |\n",
        "|-----------------------------------------------------------------|----------|\n",
        "| 1. XGBoost on numerical and categorical features                | 0.947    |\n",
        "| 2. BERT on review text only                                     | 0.898    |\n",
        "| 3. BERT on review text and converted text                       | 0.954    |\n",
        "| 4. Modified BERT combining numerical features                   | 0.890    |\n",
        "\n",
        "\n",
        "\n",
        "The preliminary results show Method 3:[Hugging Face BERT Transformer - Convert numerical and categorical to text and treat all text together](#4-Transform-Numerical-and-Categorical-to-Text-and-Use-BERT-Transformer) gives the best F1 score.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion and Recommendations\n",
        "---\n",
        "\n",
        "This note demonstrates the methodologies that can be used to treat numerical and categorical features when using BERT text classification. \n",
        "\n",
        "It is worth mentioning only 2000 samples and 2 epochs were used for the BERT based models. These model performance could be improved if more data point and epochs were used. To improve the model performance, the following actions are recommended:\n",
        "\n",
        "- Use more samples to train the BERT models\n",
        "- Use more epochs to train the BERT models\n",
        "- Optimize the BERT models by hyperparameter tuning (optimizer, dropout, learning rate, batch size, epoch, activation function, hidden layer, etc.)\n",
        "- Utilize GPU and cloud based distribution framework to expedite training process"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "---\n",
        "\n",
        "https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForSequenceClassification\n",
        "\n",
        "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
        "\n",
        "https://towardsdatascience.com/how-to-combine-textual-and-numerical-features-for-machine-learning-in-python-dc1526ca94d9\n",
        "\n",
        "https://github.com/jianzhnie/MultimodalTransformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "### 1 Dataset\n",
        "\n",
        "Women's E-Commerce Clothing Reviews is used in the current notebook\n",
        "\n",
        "This dataset is available on Kaggle [here](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://drive.google.com/uc?id=1ZYdt0zN4LjWqP3cQDblNhXjeohcryY5H\", \"Womens Clothing E-Commerce Reviews.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3TDG-juDmkPL"
      },
      "source": [
        "Read dataset to DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "Ql8mJlKAUWX5",
        "outputId": "b369af44-c916-4687-ac8b-6b3366bb1fb8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ========For demonstration purpose,and due to limited computing power, only 2000 samples are tested. ======="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.iloc[:2000,:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tUO5M8v_n86z"
      },
      "source": [
        "*Features*\n",
        "\n",
        "\"**Recommended IND**\" is the label we are trying to predict for this dataset. \"1\" means the reviewer recommended the product and \"0\" means they do not.\n",
        "\n",
        "The following are categorical features:\n",
        "\n",
        "* Division Name\n",
        "* Department Name\n",
        "* Class Name\n",
        "* Clothing ID\n",
        "\n",
        "And the following are numerical features:\n",
        "\n",
        "* Age\n",
        "* Rating\n",
        "* Positive Feedback Count\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I0NqoHqCigLL"
      },
      "source": [
        "#### Data Split"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E8DujZL2kYPb"
      },
      "source": [
        "The data will be split into train, val, and test.\n",
        "\n",
        "In order to transform data and then split the data, the indices for the three datasets are determined in advance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbBkXm5G9rZU",
        "outputId": "0a90e998-3e3c-4b5d-a6f6-27a5eaf93ac9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "val_size = int(0.1 * len(df))\n",
        "test_size = len(df) - (train_size + val_size)\n",
        "\n",
        "indeces = [_ for _ in range(0, len(df))]\n",
        "\n",
        "# Shuffle the indeces randomly.\n",
        "random.shuffle(indeces)\n",
        "\n",
        "# Get each set of indeces.\n",
        "train_idx = indeces[0:train_size]\n",
        "val_idx = indeces[train_size:(train_size + val_size)]\n",
        "test_idx = indeces[(train_size + val_size):]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-d_gmH5kNJ"
      },
      "source": [
        "### 2 Classification by Numerical and Categorical Features Using XGBoost"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MEbLVN8-qhvD"
      },
      "source": [
        "XGBoost is used for classified the user based on numerical and categorical features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytmD-iF-6ohZ",
        "outputId": "a62cf927-2890-46bf-9ec7-fdb9845e6788"
      },
      "outputs": [],
      "source": [
        "#!pip install xgboost"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ifSuYftnCbbp"
      },
      "source": [
        "**Encode Categories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "uC8N8-9V9wEn",
        "outputId": "67ce300a-5033-4811-ee05-49bff8b6d758"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features\n",
        "df[\"Clothing ID\"] = df[\"Clothing ID\"].astype('category')\n",
        "df[\"Division Name\"] = df[\"Division Name\"].astype('category')\n",
        "df[\"Department Name\"] = df[\"Department Name\"].astype('category')\n",
        "df[\"Class Name\"] = df[\"Class Name\"].astype('category')\n",
        "\n",
        "df[\"Clothing ID\"] = df[\"Clothing ID\"].cat.codes\n",
        "df[\"Division Name\"] = df[\"Division Name\"].cat.codes\n",
        "df[\"Department Name\"] = df[\"Department Name\"].cat.codes\n",
        "df[\"Class Name\"] = df[\"Class Name\"].cat.codes\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FWXIv41DtqXJ"
      },
      "source": [
        "Convert Numerical Features to Floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "LGmxjwR2JkJs",
        "outputId": "d38159aa-12b8-4210-b95a-94edbfc00077"
      },
      "outputs": [],
      "source": [
        "# Cast the numerical features to floats.\n",
        "df[\"Age\"] = df[\"Age\"].astype('float')\n",
        "df[\"Rating\"] = df[\"Rating\"].astype('float')\n",
        "df[\"Positive Feedback Count\"] = df[\"Positive Feedback Count\"].astype('float')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df=df\n",
        "processed_df.dtypes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pzTvgTQS7F9B"
      },
      "source": [
        "Prepare training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "5ghgLy7MumqF",
        "outputId": "10d16828-9779-4006-b686-dad9d80b3618"
      },
      "outputs": [],
      "source": [
        "# Retrieve the labels for each of the splits.\n",
        "y_train = df[\"Recommended IND\"].iloc[train_idx]\n",
        "y_val = df[\"Recommended IND\"].iloc[val_idx]\n",
        "y_test = df[\"Recommended IND\"].iloc[test_idx]\n",
        "\n",
        "# Before selecting the inputs, remove text columns and the labels.\n",
        "df = df.drop(columns=[\"Title\", \"Review Text\", \"Recommended IND\"])\n",
        "\n",
        "# Select the inputs for the different splits.\n",
        "X_train = df.iloc[train_idx]\n",
        "X_val = df.iloc[val_idx]\n",
        "X_test = df.iloc[test_idx]\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrT4bkgg6qWx",
        "outputId": "c20bb68e-6994-40a5-be95-e80b65f45ef9"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Create an instance of the classifier\n",
        "model = XGBClassifier()\n",
        "\n",
        "# Train it on the training set.\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the trained model to predict the labels for the test set.\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate the F1 score.\n",
        "f1 = f1_score(y_true = y_test,\n",
        "              y_pred = predictions)\n",
        "\n",
        "print('Using XGBoost on non-text features...')\n",
        "print('\\nF1: %.3f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wsMNi7EoysB6"
      },
      "source": [
        "### 3 Classification by Text Only Using Hugging Face BERT Transformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0QmFRZLywyb"
      },
      "source": [
        "The BERT applies to the text feature only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text features only.\n",
        "\n",
        "text_feats = []\n",
        "\n",
        "# The labels for the samples.\n",
        "labels = []\n",
        "\n",
        "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
        "df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
        "df = df.iloc[:2000,:]\n",
        "\n",
        "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
        "# replace the NaN values with empty string.\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "# For each of the samples...\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "    text_feats.append(row[\"Review Text\"])\n",
        "\n",
        "    # Also record the sample's label.\n",
        "    labels.append(row[\"Recommended IND\"])\n",
        "\n",
        "print('Dataset contains {:,} samples.'.format(len(text_feats)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 32\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the maximum sentence length in the dataset\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in text_feats:\n",
        "\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the maximum length of the input text.\n",
        "max_len = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize text and map the tokens to their word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "import torch\n",
        "print('Encoding all reviews in the dataset...')\n",
        "\n",
        "# For every sentence...\n",
        "for sent in text_feats:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Split the samples, and create TensorDatasets for each split. \n",
        "train_dataset = TensorDataset(input_ids[train_idx], attention_masks[train_idx], labels[train_idx])\n",
        "val_dataset = TensorDataset(input_ids[val_idx], attention_masks[val_idx], labels[val_idx])\n",
        "test_dataset = TensorDataset(input_ids[test_idx], attention_masks[test_idx], labels[test_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library \n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, \n",
        "                  eps = 1e-8 \n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples!)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if we have a GPU available\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on \n",
        "# https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        " \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "#pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap (doesn't seem to work in Colab).\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataLoader \n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1\n",
        "f1 = f1_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('F1 Score: %.3f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3XV5o0XY_4LL"
      },
      "source": [
        "\n",
        "### 4 Transform Numerical and Categorical to Text and Use BERT Transformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wAanHBNH6sQt"
      },
      "source": [
        "This section is based on this blog\n",
        "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
        "The simplest way to handle these non-text features is to convert them into text and concatenate them with the review text!\n",
        "\n",
        "\n",
        "<img src=\"./img/all_txt.png\" width=\"900\">  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLjJ2Fup6duN",
        "outputId": "2d997d1b-30d5-4f02-9c59-0d03b968568c"
      },
      "outputs": [],
      "source": [
        "# This will hold all of the dataset samples, as strings.\n",
        "sen_w_feats = []\n",
        "\n",
        "\n",
        "# The labels for the samples.\n",
        "labels = []\n",
        "\n",
        "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
        "df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
        "df=df.iloc[:2000,:]\n",
        "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
        "# replace the NaN values with empty string.\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "\n",
        "print('Combining features into strings...')\n",
        "\n",
        "# For each of the samples...\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "    # Piece it together...    \n",
        "    combined = \"\"\n",
        "    \n",
        "    #combined += \"The ID of this item is {:}, \".format(row[\"Clothing ID\"])\n",
        "    combined += \"This item comes from the {:} department and {:} division, \" \\\n",
        "                \"and is classified under {:}. \".format(row[\"Department Name\"], \n",
        "                                                       row[\"Division Name\"], \n",
        "                                                       row[\"Class Name\"])\n",
        "    \n",
        "    combined += \"I am {:} years old. \".format(row[\"Age\"])\n",
        "    \n",
        "    combined += \"I rate this item {:} out of 5 stars. \".format(row[\"Rating\"])\n",
        "    \n",
        "    # Not all samples have titles.\n",
        "    if not row[\"Title\"] == \"\":\n",
        "        combined += row[\"Title\"] + \". \"\n",
        "    \n",
        "    # Finally, append the review the text!\n",
        "    combined += row[\"Review Text\"]\n",
        "    \n",
        "    # Add the combined text to the list.\n",
        "    sen_w_feats.append(combined)\n",
        "\n",
        "    # Also record the sample's label.\n",
        "    labels.append(row[\"Recommended IND\"])\n",
        "\n",
        "print('  DONE.')\n",
        "\n",
        "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-StersG0NH-",
        "outputId": "d2c304d0-1d9d-4feb-e143-993852a7d80d"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "print(wrapper.fill(sen_w_feats[0]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "02bc328c-239b-4973-8ee0-865a8f4d668b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XBNf8drb2TRM"
      },
      "source": [
        "Load the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183,
          "referenced_widgets": [
            "5a2ca771fb704796931badc250a4710b",
            "df618c712e244395827b15606012166f",
            "4ed4b857438c4c1a8aee41706bbd0e1c",
            "efc77f65f1ec4551bf2c0d8c4d22f856",
            "204654825aa2475197970a2a2f53d743",
            "168c276a350748d1b265573e7b7a9a35",
            "cda92370c26d4ebd950ba3a02d3d27cd",
            "09cf8b8fdee14578bf22cb3593e95f9d",
            "1476269b58db4d02a4ad4b102df71452",
            "5293024422ea4142b4ddb5e9ec3cda9e",
            "55cf76835a224bf68093abfc965c6e31",
            "78f866cdb077443589a07041e6ef056c",
            "3dd5b28334414950a5c3227b59eaea75",
            "021122626be74d9095e4eba6c8f4164c",
            "283d262434de482e90f81b4d4408deaa",
            "e5b9956b9c18473a8a046d09e04605fb",
            "6c0660d2f7e14f169f5b293dd25bdb61",
            "02aeb5e9b17b4b60b1210cc26c46fbd9",
            "e78c4ad52d65444c9e8bb95ef4dc045e",
            "8f3f97f462604668a6efeb512b996510",
            "5ef82444ee8145278e135cc6dbca17c5",
            "332a2a05064342b39735f96e25d14b2e",
            "96c9b7c395a64ce6b354db2416284851",
            "e0e88078dd4f49a9921f77767ce5ae4d"
          ]
        },
        "id": "Z474sSC6oe7A",
        "outputId": "e29fc308-ce7d-48eb-81b7-11f4ad00dc7a"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WArTeY212odt"
      },
      "source": [
        "Load the BERT Classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224,
          "referenced_widgets": [
            "83b7b13e4933422cb7261d54fc419daf",
            "2c8c0207d3a4467881f35c2ac2cb16bc",
            "7d1a94b77e8a4bf68363f44c9faa2734",
            "f2c18e2fc9f24ac9b9424ddb104f7710",
            "bbd8409399fb43f484170614ba085d0b",
            "d80e61b01aeb4f88bcd5c29e1964afa2",
            "ef56fb61185649f593076ad78e6c4e19",
            "8f16b1362eca4f2e9afd9356acfdb269",
            "aacd2993ba43439496e1fa8d7848b785",
            "ac5cfb5ded434b87b8c939568c60b480",
            "31c9c14db0e342c1af016d1a5b8ed656",
            "2c2e638927724b19b0da2119193fc3ff",
            "660669d038114b4babb1b40e4987a127",
            "c8225a068137464e9b07eeb45ca383c4",
            "231703f6a0c447d8bc27bc5c5f84e1d2",
            "a2f5d5ded6444e788d635aebd856796b"
          ]
        },
        "id": "gFsCTp_mporB",
        "outputId": "c1395ef8-1874-4757-918f-3643f3c19b8d"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 2, \n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "#desc = model.cuda()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g7IzbLxb4x9s"
      },
      "source": [
        "Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3DxsSO0474t"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKsH2sU0OCQA",
        "outputId": "cd8daa78-eeb7-429f-fe9e-cfbfe61d0bac"
      },
      "outputs": [],
      "source": [
        "# Get the maximum sentence length in the dataset.\n",
        "max_len = 0\n",
        "\n",
        "\n",
        "for sent in sen_w_feats:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6HYLBalwm69"
      },
      "outputs": [],
      "source": [
        "# Set max_len \n",
        "max_len = 190"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ"
      },
      "source": [
        "## 3.3. Tokenize & Encode"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tIWAoWL2RK1p"
      },
      "source": [
        "Now we can do the real tokenization and encoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bBdb3pt8LuQ",
        "outputId": "24429110-9e9b-491d-8e24-8aad1c5338e7"
      },
      "outputs": [],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "print('Encoding all reviews in the dataset...')\n",
        "\n",
        "for sent in sen_w_feats:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('DONE.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GsLSwdUZABTR"
      },
      "source": [
        "Now that it's done, we can divide up the samples into the three splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IadUCLro_yA_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Split the samples, and create TensorDatasets for each split. \n",
        "train_dataset = TensorDataset(input_ids[train_idx], attention_masks[train_idx], labels[train_idx])\n",
        "val_dataset = TensorDataset(input_ids[val_idx], attention_masks[val_idx], labels[val_idx])\n",
        "test_dataset = TensorDataset(input_ids[test_idx], attention_masks[test_idx], labels[test_idx])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, \n",
        "                  eps = 1e-8 \n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples!)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5B99H5H2-W"
      },
      "source": [
        "Define a helper function for calculating simple accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KNhRtWPXH9C3"
      },
      "source": [
        "Helper function for formatting elapsed times as `hh:mm:ss`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tywzf6EA2fX"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "5bd5fe1c-0751-42c0-f40e-937c995314a1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "   \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        " \n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "je7y6-105Vp8"
      },
      "source": [
        "Training Results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hba10sXR7Xi6",
        "outputId": "bbac8a45-3d39-4d90-e614-6147f1ea4ad5"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader \n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5373-b2zkNl"
      },
      "outputs": [],
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCYZa1lQ8Jn8",
        "outputId": "b8c74945-b0bf-41a9-9921-50dae8936a5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1\n",
        "f1 = f1_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('F1 Score: %.3f' % f1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BfJbda1Mzu3y"
      },
      "source": [
        "Here are the final scores:\n",
        "\n",
        "| Strategy                             | F1 Score |\n",
        "|--------------------------------------|----------|\n",
        "| Always predict “recommended”         | 0.906    |\n",
        "| Predict “recommended” if rating >= 3 | 0.953    |\n",
        "| XGBoost                              | 0.965    |\n",
        "| BERT on review text                  | 0.945    |\n",
        "| BERT, all features to text           | 0.968    |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 BERT Text Classification Combining Numerical and Categorical Features\n",
        "\n",
        "The customized model is based on BertForSequenceClassification\n",
        "https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForSequenceClassification\n",
        "\n",
        "<img src=\"./img/txt_num.png\" width=\"900\">  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "class CustBertClassifier2(nn.Module):\n",
        "    def __init__(self, hidden_size, dense_size, numeric_feature_size, output_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Use pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attentions=True)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.weights = nn.Parameter(torch.rand(13, 1))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, dense_size)\n",
        "        self.fc2 = nn.Linear(dense_size + numeric_feature_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, input_num, attention_mask, labels):        \n",
        "        all_hidden_states, all_attentions = self.bert(input_ids,attention_mask=attention_mask)[-2:]\n",
        "        outputs=self.bert(input_ids,attention_mask=attention_mask)\n",
        "        \n",
        "        batch_size = input_ids.shape[0]\n",
        "        ht_cls = torch.cat(all_hidden_states)[:, :1, :].view(13, batch_size, 1, 768)\n",
        "        atten = torch.sum(ht_cls * self.weights.view(13, 1, 1, 1), dim=[1, 3])\n",
        "        atten = F.softmax(atten.view(-1), dim=0)\n",
        "        feature = torch.sum(ht_cls * atten.view(13, 1, 1, 1), dim=[0, 2])\n",
        "        dense_out = self.fc1(self.dropout(feature))\n",
        "        concat_layer = torch.cat((dense_out, input_num.float()), 1)\n",
        "        logits = self.fc2(concat_layer)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  \n",
        "\n",
        "        if labels is not None:\n",
        "            if self.output_size == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.output_size), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will hold text features\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "text_feats = []\n",
        "\n",
        "# The labels for the samples.\n",
        "labels = []\n",
        "\n",
        "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
        "df = pd.read_csv('Womens Clothing E-Commerce Reviews.csv', index_col=0)\n",
        "df = df.iloc[:2000,:]\n",
        "\n",
        "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
        "# replace the NaN values with empty string.\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "# For each of the samples...\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "    text_feats.append(row[\"Review Text\"])\n",
        "\n",
        "    # Also record the sample's label.\n",
        "    labels.append(row[\"Recommended IND\"])\n",
        "\n",
        "print('  DONE.')\n",
        "\n",
        "print('Dataset contains {:,} samples.'.format(len(text_feats)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in text_feats:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len=150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "import torch\n",
        "print('Encoding all reviews in the dataset...')\n",
        "\n",
        "# For every sentence...\n",
        "for sent in text_feats:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the samples, and create TensorDatasets for each split. \n",
        "X_train_txt = input_ids[train_idx]\n",
        "X_val_txt = input_ids[val_idx]\n",
        "X_test_txt = input_ids[test_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the dataframe from XGBoost section\n",
        "y_train = processed_df[\"Recommended IND\"].iloc[train_idx].tolist()\n",
        "y_val = processed_df[\"Recommended IND\"].iloc[val_idx].tolist()\n",
        "y_test = processed_df[\"Recommended IND\"].iloc[test_idx].tolist()\n",
        "\n",
        "# Before selecting the inputs, remove text columns and the labels.\n",
        "df2 = processed_df.drop(columns=[\"Title\", \"Review Text\", \"Recommended IND\"])\n",
        "\n",
        "# Select the non-text inputs for the different splits.\n",
        "X_train_num = pd.DataFrame(df2.iloc[train_idx])\n",
        "X_val_num = pd.DataFrame(df2.iloc[val_idx])\n",
        "X_test_num = pd.DataFrame(df2.iloc[test_idx])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_num.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 32\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_num=torch.tensor(X_train_num.values)\n",
        "X_test_num=torch.tensor(X_test_num.values)\n",
        "X_val_num=torch.tensor(X_val_num.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Split the samples, and create TensorDatasets for each split. \n",
        "train_dataset = TensorDataset(input_ids[train_idx], X_train_num , attention_masks[train_idx],labels[train_idx])\n",
        "val_dataset = TensorDataset(input_ids[val_idx], X_val_num , attention_masks[val_idx], labels[val_idx])\n",
        "test_dataset = TensorDataset(input_ids[test_idx], X_test_num , attention_masks[test_idx],labels[test_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model\n",
        "#(self, hidden_size, dense_size, numeric_feature_size, output_size, dropout=0.1)\n",
        "model = CustBertClassifier2(768, 8, 7, 2, dropout=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random, time\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the following:\n",
        "# https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/#24-bert-on-review-text-only\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "    \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_num_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "   \n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        result = model(b_input_ids, b_num_ids, b_input_mask, b_labels)\n",
        "\n",
        "        #print(result)\n",
        "\n",
        "        logits = result[1]\n",
        "        loss = result[0]\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "        \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "     \n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)           \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_num_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, b_num_ids, b_input_mask, b_labels)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "    \n",
        "        loss = result[0]\n",
        "        logits = result[1]\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(X_val_txt)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(X_val_txt)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    \n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataLoader to batch our test samples\n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "print('Predicting labels for {:,} samples...'.format(len(test_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_num_ids, b_input_mask,b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, b_num_ids, b_input_mask,b_labels)\n",
        "\n",
        "  logits = result[1]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(predictions[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1\n",
        "f1 = f1_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('F1 Score: %.3f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to CSV.\n",
        "pd.DataFrame(np.stack((flat_predictions,flat_true_labels), axis=1), columns=[\"predictions\", \"labels\"]).to_csv(\"predictions.csv\", index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "predictions=pd.read_csv(\"predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1\n",
        "f1 = f1_score(predictions[\"labels\"],predictions[\"predictions\"])\n",
        "\n",
        "print('F1 Score: %.3f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "021122626be74d9095e4eba6c8f4164c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02aeb5e9b17b4b60b1210cc26c46fbd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09cf8b8fdee14578bf22cb3593e95f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1476269b58db4d02a4ad4b102df71452": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55cf76835a224bf68093abfc965c6e31",
              "IPY_MODEL_78f866cdb077443589a07041e6ef056c"
            ],
            "layout": "IPY_MODEL_5293024422ea4142b4ddb5e9ec3cda9e"
          }
        },
        "168c276a350748d1b265573e7b7a9a35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "204654825aa2475197970a2a2f53d743": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "231703f6a0c447d8bc27bc5c5f84e1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "283d262434de482e90f81b4d4408deaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c2e638927724b19b0da2119193fc3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f5d5ded6444e788d635aebd856796b",
            "placeholder": "​",
            "style": "IPY_MODEL_231703f6a0c447d8bc27bc5c5f84e1d2",
            "value": " 440M/440M [00:09&lt;00:00, 48.9MB/s]"
          }
        },
        "2c8c0207d3a4467881f35c2ac2cb16bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c9c14db0e342c1af016d1a5b8ed656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8225a068137464e9b07eeb45ca383c4",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_660669d038114b4babb1b40e4987a127",
            "value": 440473133
          }
        },
        "332a2a05064342b39735f96e25d14b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dd5b28334414950a5c3227b59eaea75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4ed4b857438c4c1a8aee41706bbd0e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_168c276a350748d1b265573e7b7a9a35",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_204654825aa2475197970a2a2f53d743",
            "value": 231508
          }
        },
        "5293024422ea4142b4ddb5e9ec3cda9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55cf76835a224bf68093abfc965c6e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_021122626be74d9095e4eba6c8f4164c",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dd5b28334414950a5c3227b59eaea75",
            "value": 28
          }
        },
        "5a2ca771fb704796931badc250a4710b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ed4b857438c4c1a8aee41706bbd0e1c",
              "IPY_MODEL_efc77f65f1ec4551bf2c0d8c4d22f856"
            ],
            "layout": "IPY_MODEL_df618c712e244395827b15606012166f"
          }
        },
        "5ef82444ee8145278e135cc6dbca17c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "660669d038114b4babb1b40e4987a127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "6c0660d2f7e14f169f5b293dd25bdb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e78c4ad52d65444c9e8bb95ef4dc045e",
              "IPY_MODEL_8f3f97f462604668a6efeb512b996510"
            ],
            "layout": "IPY_MODEL_02aeb5e9b17b4b60b1210cc26c46fbd9"
          }
        },
        "78f866cdb077443589a07041e6ef056c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b9956b9c18473a8a046d09e04605fb",
            "placeholder": "​",
            "style": "IPY_MODEL_283d262434de482e90f81b4d4408deaa",
            "value": " 28.0/28.0 [00:00&lt;00:00, 50.0B/s]"
          }
        },
        "7d1a94b77e8a4bf68363f44c9faa2734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d80e61b01aeb4f88bcd5c29e1964afa2",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbd8409399fb43f484170614ba085d0b",
            "value": 570
          }
        },
        "83b7b13e4933422cb7261d54fc419daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d1a94b77e8a4bf68363f44c9faa2734",
              "IPY_MODEL_f2c18e2fc9f24ac9b9424ddb104f7710"
            ],
            "layout": "IPY_MODEL_2c8c0207d3a4467881f35c2ac2cb16bc"
          }
        },
        "8f16b1362eca4f2e9afd9356acfdb269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3f97f462604668a6efeb512b996510": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0e88078dd4f49a9921f77767ce5ae4d",
            "placeholder": "​",
            "style": "IPY_MODEL_96c9b7c395a64ce6b354db2416284851",
            "value": " 466k/466k [00:00&lt;00:00, 2.78MB/s]"
          }
        },
        "96c9b7c395a64ce6b354db2416284851": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2f5d5ded6444e788d635aebd856796b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aacd2993ba43439496e1fa8d7848b785": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31c9c14db0e342c1af016d1a5b8ed656",
              "IPY_MODEL_2c2e638927724b19b0da2119193fc3ff"
            ],
            "layout": "IPY_MODEL_ac5cfb5ded434b87b8c939568c60b480"
          }
        },
        "ac5cfb5ded434b87b8c939568c60b480": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd8409399fb43f484170614ba085d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "c8225a068137464e9b07eeb45ca383c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda92370c26d4ebd950ba3a02d3d27cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d80e61b01aeb4f88bcd5c29e1964afa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df618c712e244395827b15606012166f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e88078dd4f49a9921f77767ce5ae4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b9956b9c18473a8a046d09e04605fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e78c4ad52d65444c9e8bb95ef4dc045e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_332a2a05064342b39735f96e25d14b2e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ef82444ee8145278e135cc6dbca17c5",
            "value": 466062
          }
        },
        "ef56fb61185649f593076ad78e6c4e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efc77f65f1ec4551bf2c0d8c4d22f856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09cf8b8fdee14578bf22cb3593e95f9d",
            "placeholder": "​",
            "style": "IPY_MODEL_cda92370c26d4ebd950ba3a02d3d27cd",
            "value": " 232k/232k [00:01&lt;00:00, 127kB/s]"
          }
        },
        "f2c18e2fc9f24ac9b9424ddb104f7710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f16b1362eca4f2e9afd9356acfdb269",
            "placeholder": "​",
            "style": "IPY_MODEL_ef56fb61185649f593076ad78e6c4e19",
            "value": " 570/570 [00:09&lt;00:00, 59.8B/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
