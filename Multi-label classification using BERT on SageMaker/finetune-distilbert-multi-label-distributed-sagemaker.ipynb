{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Hugging Face DistilBERT model for multi-label text classification on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ( *Distributed training, data parallelism or model parallelism, is optional. It can be turned on by uncommenting relevant sections.* )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will use the Hugging Faces `transformers` and `datasets` library with Amazon SageMaker to fine-tune a pre-trained transformer on multi-label text classification dataset. \n",
    "\n",
    "You will then deploy the resulting model for inference using SageMaker Endpoint.\n",
    "\n",
    "This notebook is modified from [this example.](https://github.com/aws-samples/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face/blob/main/finetune-distilbert.ipynb)\n",
    "\n",
    "### The model\n",
    "\n",
    "You'll be using an offshoot of [BERT](https://arxiv.org/abs/1810.04805) called [DistilBERT](https://arxiv.org/abs/1910.01108) that is smaller, and so faster and cheaper for both training and inference. A pre-trained model is available in the [`transformers`](https://huggingface.co/transformers/) library from [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "### The data\n",
    "\n",
    "The [Twitter multi-label dateset](https://huggingface.co/maxpe/twitter-roberta-base-jun2022_sem_eval_2018_task_1) is used. One twitter text could have multiple labels.\n",
    "\n",
    "```\n",
    "\"text\": \"I'm very happy I fine-tune a hugging face DistilBert model for multi-label classification\"\n",
    "\"label\": [\"joy\", \"optimism\"]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Dependencies\n",
    "Install the dependencies required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"sagemaker>=2.48.0\" --upgrade\n",
    "!pip install -qq sagemaker-huggingface-inference-toolkit \n",
    "!pip install -qq ipywidgets\n",
    "!pip install -qq watermark \n",
    "!pip install -qq \"seaborn>=0.11.0\"\n",
    "!pip install -qq transformers \"datasets[s3]==1.18.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this on a SageMaker environment, make sure to reboot the Kernel via the dropdown menu at the top after you've installed the above dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import ProfilerConfig, DebuggerHookConfig, Rule, ProfilerRule, rule_configs\n",
    "import sagemaker.huggingface\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from textwrap import wrap\n",
    "\n",
    "import boto3\n",
    "import pprint\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize'] = 17, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up SageMaker session and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local environment, if not using SageMaker/Studio Notebook.\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(\n",
    "        RoleName='AmazonSageMaker-ExecutionRole')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "The data preparation is straightforward as you're using the `datasets` library to download and preprocess the `\n",
    "amazon_polarity` dataset directly from Hugging face. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list that contains the labels. And create two dict to map labels to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset to be used with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify model to be used and define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data \n",
    "[reference](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=AFWlSsbZaRLc)\n",
    "1. Tokenize the text feature\n",
    "2. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' BCEWithLogitsLoss (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"Tweet\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = encoded_dataset['train']\n",
    "test_dataset = encoded_dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# Upload to S3\n",
    "s3 = S3FileSystem()\n",
    "s3_prefix = f'samples/datasets/twitter'\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)\n",
    "\n",
    "print(f'Uploaded training data to {training_input_path}')\n",
    "print(f'Uploaded testing data to {test_input_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job you need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator you define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in.\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py38\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    compiler_config=compiler_config,  # the compiler configuration used in the training job\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config = False, \n",
    "    max_run=36000,  # expected max run in seconds\n",
    ")\n",
    "```\n",
    "When you create a SageMaker training job, SageMaker takes care of starting and managing all the required compute instances with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container local storage at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 5 --model_name distilbert-base-uncased --token_name distilbert-base-cased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. The training script expect the `HuggingFace` model and token name so it can retrieve them.\n",
    "\n",
    "Sagemaker is providing other useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimatorâ€™s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py is modified from https://github.com/aws-samples/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face/blob/main/scripts/train.py\n",
    "# https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=797b2WHJqUgZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile train.py\n",
    "\n",
    "\"\"\"\n",
    "Training script for Hugging Face SageMaker Estimator\n",
    "\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "#from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_from_disk, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# for model parallelism, need to use sagemaker specific trainer\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\n",
    "from transformers.sagemaker import SageMakerTrainer as Trainer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    # is needed for Amazon SageMaker Training Compiler\n",
    "    os.environ[\"GPU_NUM_DEVICES\"] = args.n_gpus\n",
    "    \n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\"INFO\"),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "    test_dataset = load_from_disk(args.test_dir)\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    # https://engineering.freeagent.com/2021/09/15/fine-tuning-bert-for-multiclass-categorisation-with-amazon-sagemaker/\n",
    "  \n",
    "    def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "        # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(torch.Tensor(predictions))\n",
    "        # next, use threshold to turn them into integer predictions\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        y_pred[np.where(probs >= threshold)] = 1\n",
    "        # finally, compute metrics\n",
    "        y_true = labels\n",
    "        f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "        roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        # return as dictionary\n",
    "        metrics = {'f1': f1_micro_average,\n",
    "                   'roc_auc': roc_auc,\n",
    "                   'accuracy': accuracy}\n",
    "        return metrics\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        print('labels:',labels, labels.shape)\n",
    "        print('preds:',preds, preds.shape)\n",
    "        \n",
    "        result = multi_label_metrics(\n",
    "            predictions=preds, \n",
    "            labels=labels)\n",
    "        return result\n",
    "\n",
    "    # labels is a list\n",
    "    labels = train_dataset[0][\"labels\"]\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    label2id={'anger': 0,\n",
    "              'anticipation': 1,\n",
    "              'disgust': 2,\n",
    "              'fear': 3,\n",
    "              'joy': 4,\n",
    "              'love': 5,\n",
    "              'optimism': 6,\n",
    "              'pessimism': 7,\n",
    "              'sadness': 8,\n",
    "              'surprise': 9,\n",
    "              'trust': 10}\n",
    "    id2label={0: 'anger',\n",
    "              1: 'anticipation',\n",
    "              2: 'disgust',\n",
    "              3: 'fear',\n",
    "              4: 'joy',\n",
    "              5: 'love',\n",
    "              6: 'optimism',\n",
    "              7: 'pessimism',\n",
    "              8: 'sadness',\n",
    "              9: 'surprise',\n",
    "              10: 'trust'}\n",
    "   \n",
    "\n",
    "    # download model and tokenizer from model hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, problem_type=\"multi_label_classification\", num_labels=num_labels,id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "        #load_best_model_at_end=True,\n",
    "        #metric_for_best_model=\"f1\",\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.train()\n",
    "\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your training job so you can follow it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "ct = datetime.datetime.now() \n",
    "current_time = str(ct.now()).replace(\":\", \"-\").replace(\" \", \"-\")[:19]\n",
    "training_job_name=f'finetune-{model_name}-{current_time}'\n",
    "print( training_job_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# initialize the Amazon Training Compiler\n",
    "compiler_config=TrainingCompilerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size limits\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html#training-compiler-supported-frameworks-pytorch\n",
    "# Using p3.2xlarge for bert-base-uncased, batch limits:  16 or 24 (compiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'epochs': 3,\n",
    "                 'train_batch_size': 24,\n",
    "                 'eval_batch_size': 24,   \n",
    "                 'model_name': model_name,\n",
    "                 'learning_rate': 3e-5,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit metrics to cloudwatch\n",
    "metric_definitions=[\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed training (Optional)\n",
    "https://sagemaker.readthedocs.io/en/v2.59.8/frameworks/huggingface/sagemaker.huggingface.html\n",
    "\n",
    "The 'distribution' dictionary with information on how to run distributed training (default: None). Currently, the following are supported: distributed training with parameter servers, SageMaker Distributed (SMD) Data and Model Parallelism, and MPI. SMD Model Parallelism can only be used with MPI. To enable parameter server use the following setup:\n",
    "```\n",
    "{\n",
    "    \"parameter_server\": {\n",
    "        \"enabled\": True\n",
    "    }\n",
    "}\n",
    "```\n",
    "To enable MPI:\n",
    "```\n",
    "{\n",
    "    \"mpi\": {\n",
    "        \"enabled\": True\n",
    "    }\n",
    "}\n",
    "```\n",
    "To enable SMDistributed Data Parallel or Model Parallel:\n",
    "```\n",
    "{\n",
    "    \"smdistributed\": {\n",
    "        \"dataparallel\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"modelparallel\": {\n",
    "            \"enabled\": True,\n",
    "            \"parameters\": {}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to the train.py if using parallelism\n",
    "\n",
    "\n",
    "\n",
    "[Distributed Training: Data-Parallel](https://huggingface.co/transformers/v4.4.2/sagemaker.html#prepare-a-transformers-fine-tuning-script)\n",
    "You can use SageMaker Data Parallelism Library out of the box for distributed training. We added the functionality of Data Parallelism directly into the Trainer. If your train.py uses the Trainer API you only need to define the distribution parameter in the HuggingFace Estimator.\n",
    "\n",
    "[Distributed Training: Model-Parallel](https://huggingface.co/transformers/v4.4.2/sagemaker.html#prepare-a-transformers-fine-tuning-script)\n",
    "You can use SageMaker Model Parallelism Library out of the box for distributed training. We extended the Trainer API to the SageMakerTrainer to use the model parallelism library. Therefore you only have to change the imports in your train.py.\n",
    "\n",
    "[Example Notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)\n",
    "\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\n",
    "from transformers.sagemaker import SageMakerTrainer as Trainer\n",
    "After the adjustments in the train.py you need to extend the distribution configuration in the HuggingFace Estimator. For detailed information about the adjustments take a look [here.](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters)\n",
    "\n",
    "[Model parallelism basics](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution_dataparallel = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please note smdataparallel supports instance types:('ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p4de.24xlarge')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for running training on smdistributed Model Parallel\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 1,\n",
    "}\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 1,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"partitions\": 2,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution_modelparallel={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please note smmodelparallel does not support SageMaker training compiler. 5/10/2023*\n",
    "\n",
    "fp16 was not set up for model parallel, otherwise gave error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance configurations\n",
    "instance_type='ml.p3.2xlarge'\n",
    "instance_count=2\n",
    "instance_volume_size=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SageMaker Hugging Face Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html#training-compiler-supported-frameworks\n",
    "# 05/10/2023\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py38\",\n",
    "    volume_size=instance_volume_size,\n",
    "    hyperparameters=hyperparameters,\n",
    "    #compiler_config=compiler_config,  # the compiler does not work with distributed model parallel.\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config = False, \n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=36000,  # expected max run in seconds\n",
    "    distribution= distribution_modelparallel, #comment out if not using distributed model parallel\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts the training job using the estimator fit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path}, job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the training to finish. Training takes approximately 10 mins to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training metrics\n",
    "You can now display the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# Captured metrics can be accessed as a Pandas dataframe\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers pipelines` API allows you to use the `pipelines` features. \n",
    "\n",
    "The API is oriented at the API of the [ðŸ¤—  Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html), meaning your inputs need to be defined in the `inputs` key and if you want additional supported `pipelines` parameters you can add them in the `parameters` key. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now proceed and create an endpoint with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"inputs\": \"Tiller and breezy should do a collab album. Rapping and singing prolly be fire\"}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current inference image predicts one label with the highest score. The inference.py within the image can be modified to display multiple labels with scores. Alternatively, you can pack new custom inference.py with model artifact and deploy to endpoint. In this way, it can predict multiple labels.  \n",
    "\n",
    "Here're references:\n",
    "\n",
    "https://huggingface.co/docs/sagemaker/inference\n",
    "\n",
    "https://discuss.huggingface.co/t/inference-toolkit-init-and-default-template-for-custom-inference/10469\n",
    "\n",
    "https://github.com/philschmid/sample-custom-inference-sagemaker-huggingface/blob/master/code/inference.py\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/sagemaker/23_stable_diffusion_inference/sagemaker-notebook.ipynb\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Create new inference.py and deploy to endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get model artifact from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "#sm_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_url=response['ModelArtifacts']['S3ModelArtifacts']\n",
    "model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $model_url model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from sagemaker_huggingface_inference_toolkit import decoder_encoder\n",
    "import torch\n",
    "\n",
    "id2label = {\n",
    "    0: \"anger\",\n",
    "    1: \"anticipation\",\n",
    "    2: \"disgust\",\n",
    "    3: \"fear\",\n",
    "    4: \"joy\",\n",
    "    5: \"love\",\n",
    "    6: \"optimism\",\n",
    "    7: \"pessimism\",\n",
    "    8: \"sadness\",\n",
    "    9: \"surprise\",\n",
    "    10: \"trust\",\n",
    "}\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    return {\"model\": model, \"tokenizer\": tokenizer}\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    decoded_input_data = decoder_encoder.decode(input_data, content_type)\n",
    "    return decoded_input_data\n",
    "\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    inputs = model[\"tokenizer\"](\n",
    "        data[\"inputs\"],\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model[\"model\"](*tuple(inputs.values()))[0]\n",
    "\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    predictions[np.where(probs >= 0.5)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [\n",
    "        id2label[idx] for idx, label in enumerate(predictions) if label == 1.0\n",
    "    ]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pack model artifact with new inference.py and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path    \n",
    "import random\n",
    "\n",
    "# create model dir\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf model.tar.gz -C model-40770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)    \n",
    "    os.chdir(parent_dir)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri=S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/samples/datasets/twitter/bert-model\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3_model_uri model2.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the custom Hugging Face Model to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17.0\",  # transformers version used\n",
    "   pytorch_version=\"1.10.2\",        # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "new_predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"inputs\": \"I'm very happy I fine-tune a hugging face DistilBert model for multi-label classification\"}\n",
    "\n",
    "# request\n",
    "new_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "After you are finished experimenting with this notebook, run the following cell to delete the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()\n",
    "new_predictor.delete_model()\n",
    "new_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    " \n",
    "https://github.com/aws-samples/finetune-deploy-bert-with-amazon-sagemaker-for-hugging-face/blob/main/finetune-distilbert.ipynb\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/sagemaker/15_training_compiler/sagemaker-notebook.ipynb\n",
    "\n",
    "\n",
    "https://github.com/DhavalTaunk08/NLP_scripts/blob/master/Transformers_multilabel_distilbert.ipynb\n",
    "\n",
    "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=bIH9NP0MZ6-O\n",
    "\n",
    "https://engineering.freeagent.com/2021/09/15/fine-tuning-bert-for-multiclass-categorisation-with-amazon-sagemaker/\n",
    "\n",
    "https://www.alexanderjunge.net/blog/til-multi-label-automodelforsequenceclassification/\n",
    "\n",
    "https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
